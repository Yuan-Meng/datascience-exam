{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n",
    "<a href=\"http://bombora.com\"><img src=\"https://app.box.com/shared/static/e0j9v1xjmubit0inthhgv3llwnoansjp.png\" width=\"200px\" class=\"pull-right\" style=\"display: inline-block; margin: 5px; vertical-align: middle;\"></a>\n",
    "    <h1> Bombora Data Science: <br> <b>Interview Exam</b> </h1>\n",
    "</div>\n",
    "\n",
    "<img width=\"200px\" src=\"https://app.box.com/shared/static/15slg1mvjd1zldbg3xkj9picjkmhzpa5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Welcome\n",
    "\n",
    "Welcome! This notebook contains interview exam questions referenced in the *Instructions* section in the `README.md`—please read that first, *before* attempting to answer questions here.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">ADVICE</p>\n",
    "    <p>Do not read these questions and panic before reading the instructions in <code>README.md</code>.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">WARNING</p>\n",
    "\n",
    "<p>If using <a href=\"https://try.jupyter.org\">try.jupyter.org</a> do not rely on the server for anything you want to last - your server will be <span style=\"font-weight:bold\">deleted after 10 minutes of inactivity</span>. Save often and remember download notebook when you step away (you can always re-upload and start again)!</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Have fun!\n",
    "\n",
    "Regardless of outcome, getting to know you is important. Give it your best shot and we'll look forward to following up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Solutions by Yuan Meng (yuan_meng@berkeley.edu)\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">NOTE</p>\n",
    "<p>Below are answers to my 3 questions of choice. I affirm that I completed this exam independently without another person's help.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algo + Data Structures → Q 1.1 (Fibonacci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1.1 - Write a function to return the $n$th Fibonacci number\n",
    "The most straight forward way to compute the $n$th Fibonacci number is through **recursion**: As long as $n>2$, we can express it in terms of the previous two terms, $F_n = F_{n-1} + F_{n-2}$; when we hit the two base cases $F_1 = 1$ and $F_2 = 1$, our function will start computing the results all the way back to $n$. Below is my implementation using a one-liner if-else statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    \"\"\"return the nth Fibonacci number in O(2^n) time\"\"\"\n",
    "    return 1 if n <= 2 else fibonacci(n - 1) + fibonacci(n - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1.2 - Complexity analysis\n",
    "While simple, the approach above is not very efficient due to lots of **repeated computations** (e.g., when computing $F_6$, we need to redo all the computations already done for $F_5$). \n",
    "- **Time complexity is $O(2^n)$**: When we call this function on $n$, it will fork into two branches each level we go down (e.g., $F_n = F_{n-1} + F_{n-2}$, $F_{n-1} = F_{n-2} + F_{n-3}$, etc.); from $F_n$ it takes $(n-1)$ levels to reach $F_1$. The time complexity would be $O(branches^{depth}) = O(2^{n-1})$, which is equivalent to $O(2^n)$ (if we drop the $\\frac{1}{2}$ in $\\frac{1}{2}\\cdot 2^n$).\n",
    "- **Space complexity is $O(n)$**: When the function is called, it creates a recursive call stack with $(n-1)$ calls before reaching $F_1$ (takes $O(1)$ to return), so the space complexity is $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1.3 - Write a better function to return the $n$th Fibonacci number\n",
    "To make the recursive algorithm more efficient, we can **cache computations already done** so we can directly return their results upon future requests. This way, we only need to carry out the same computation once. Since we need cached computations outside of the current function call, I'll use a global variable `memo` to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable to cache computations already done\n",
    "memo = {}\n",
    "\n",
    "\n",
    "def fibonacci2(n):\n",
    "    \"\"\"return the nth Fibonacci number in O(n) time\"\"\"\n",
    "\n",
    "    # use the global cache\n",
    "    global memo\n",
    "\n",
    "    # if already computed, use cached result\n",
    "    if n in memo:\n",
    "        result = memo[n]\n",
    "    # otherwise compute and save new result\n",
    "    else:\n",
    "        result = 1 if n <= 2 else fibonacci2(n - 1) + fibonacci2(n - 2)\n",
    "        memo[n] = result\n",
    "\n",
    "    # return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1.4 - Complexity analysis\n",
    "- **Time complexity is $O(n)$**: In the function above, since we compute each Fibonacci number from $F_1$ to $F_n$ exactly once, the time complexity is just $O(n)$.\n",
    "- **Space complexity is $O(n)$**: Since each value from $F_1$ to $F_n$ is cached exactly once in this algorithm, the space complexity is also $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1.5 - Examples of optimizations\n",
    "The technique I used is called **\"memoization\"**, which is a common method for **optimizing exponential time recursive algorithms** and is used a lot in **dynamic programming**. It speeds up a program by avoiding repeated work but may require extra memory (not in this case, though) to cache previous results. In practice, we can buy computational power but not time, so it makes a lot of sense to optimize for time complexity at a reasonable cost of space complexity.\n",
    "\n",
    "More broadly, this questions asks for examples of optimizations that could improve computational performance. The topic of program optimization is so broad that I can only offer a few examples here. For instance, memoization that I talked about above belongs to a family of optimization techniques called **\"caching\"**, which is crucial in program optimization. Another example is to **use appropriate data structures** for a given problem in a given language. In Python, for instance, it's faster to grow a list by appending new items to it than concatenating multiple lists, even though the results look similar. This requires us to know language-specific implementations really well so we can apply more efficient methods. Last but not least, we can take advantage of clever **algorithm design techniques** to simplify complex problems, such as using **approximation** when exact computation is difficult (e.g., using MCMC samples to estimate Bayesian posteriors), **distributing computations** across multiple machines for parallelizable problems (e.g., using Spark to train a random forest estimator), breaking down a problem into sub-problems (e.g., divide and conquer, dynamic programming), focusing on **local optimizations** at each step (e.g., greedy method), and so on. At the end of the day, we also need to consider if all the efforts that go into optimization are worth the time saved or not and strike a balance between simplicity and optimality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prob + Stats → Q 2.2 (Making a 6-side die roll a 7?)\n",
    "This question asks us to generate a random number between 1 to 7 using a single 6-side die. We can use **von Neumann correction** to solve this problem  — \n",
    "- Roll the 6-side die twice, which can generate **36 equally likely results** that are combinations of the two events, ranging from (1, 1) to (6, 6)\n",
    "- Decide that **if we get a certain result** (e.g., (6, 6)), we'll **discard it** and repeat the previous step until we get a valid result\n",
    "- Among the 35 valid results, we can **map each 5 results to each of the numbers from 1 to 7**; this way, we'll have an equal chance of getting any number between 1 and 7\n",
    "\n",
    "This solution has a potential problem: We may get unlucky and always have invalid results. Most likely, though, we'll get a valid result in a few attempts, so this solution works well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conceptual ML → Q 3.2 (Model selection and assessment)\n",
    "**Bottom line**: Since the number of features far exceeds that of observations (10,000 vs. 100!), we must carefully avoid **overfitting** when training the multiclass classifier the question asks for.\n",
    "\n",
    "### Feature selection\n",
    "It would be disastrous to use all the features to build a classifier; apart form overfitting, we'll likely suffer from the curse of dimensionality, long training time, and low model interpretability. Common feature selection techniques fall under 3 broad categories (which I summarized in [my ML notes](https://www.notion.so/core-ml-1930f2267ce942c984b005c1bb62d429)):\n",
    "\n",
    "- **Wrapper methods**: train our model on subsets of features and use the best-performing subset in the final model\n",
    "- **Filter methods**: select features that correlate well (or have high mutual information) with the target (class labels)\n",
    "- **Embedded methods**: use a LASSO to compute feature importances and only keep features with non-zero regression coefficients \n",
    "\n",
    "This question asks us to find a \"good\" subset of features that show strong **univariate** correlation with class labels. **Filter methods** serve this purpose well. We can **compute the correlation between each feature and the target** and only select features whose Pearson's coefficient $r$ is greater than .7 or less than -.7, for example. Alternatively, we can include all the features in a **LASSO logistic model** and drop the ones with zero coefficients. Like ordinary logistic regression, LASSO doesn't pick up on relationships among variables and is therefore still focused on each feature's univariate correlation with the target. LASSO works the best if most features have zero coefficients; otherwise, we wouldn't be able to reduce the feature space significantly.\n",
    "\n",
    "Personally, I would use **dimensionality reduction** techniques such as **PCA** or **UMAP** to project data onto a lower dimensional space and do the modeling work there, so we won't have to lose potential non-linear relationships between features. Moreover, if the problem is in a domain I'm familiar with, I'll also use my **domain knowledge** to select features that make the most sense.\n",
    "\n",
    "### What models should we use?\n",
    "Given we don't have many observations, even if we can reduce the feature space somewhat, we should still refrain from using complex models that are prone to overfitting, such as boosting trees (e.g., AdaBoost, XGBoost, LightGBM, CatBoost) and neural nets, or models that don't have the capacity to handle high-dimensional data, such as kNN. These low bias/high variance models mimic patterns in observed data too faithfully to generalize to new data. By contrast, **simple models that have high bias but low variance** (e.g., **multinomial logistic regression**, **Naive Bayes**, **linear SVM**) or **bagging models** (e.g., **random forests**) that designed to reduce variance are much more appropriate for a dataset with many features but few observations.\n",
    "\n",
    "### Model tuning and training\n",
    "When the dataset is reasonably large, a common approach is to split it 3 ways into a training set (e.g., 60%), a validation set (20%), and a test set (e.g., 20%). The training set is used to train the model, the validation set is used to tune hyperparameters, and the test set is used to test how well the trained model generalizes to unseen data. \n",
    "\n",
    "However, since we don't have much data here, we can **split the dataset into a training set (e.g., 80%) and test set (e.g., 20%)**. Then we can use **k-fold cross-validation** to tune hyperparameters using the training data and then train the best estimator on the full training set. The last step is the same, which is using the test set to evaluate model performance. \n",
    "\n",
    "Depending on the nature of the data, there may be special considerations for data splitting. For instance, if we have multiple observations from the same person, then we shouldn't split one person's data into training and test sets. This is because data from one person is presumably more similar than that from different people, we may end up overestimate how well our model generalizes. In this case, we can use **group k-fold** to ensure that data from the same group (could be a person) remains in the same set. If there is an apparent **class imbalance**, we may which to preserve the same class structure in both the training and the test sets by using **stratified k-fold**. If the data is a **time series** and we wish to use the past to predict the future, then we should put past data into the training set and future data in the test set. The general idea is that we should know the data well in order to split it in a reasonable way.\n",
    "\n",
    "As mentioned, model selection or hyperparameter tuning is done using **cross-validation on the training set**. If we use k-fold cross-validation, we split the training data into k partitions, each time training a model with different hyperparameter values on (k - 1) partitions and evaluating it on the remaining one. Whichever combination minimizes our chosen error metric(s) averaged across k iterations will be used to train our final model. An extreme version of k-fold cross validation is **leave-one-out cross validation**: We train the model on all of the training data except for one data point and test the model on that data point. **For a small dataset like this one, leave-one-out is the better way to go**: Each iteration the model is trained on almost the entire training set so it reduces bias, which is valuable for learning from small datasets.\n",
    "\n",
    "### Validation and generalization\n",
    "Say we a random forest classifier tuned and trained on the training data, we can use it to predict class labels in the test data to see how well it generalizes to unseen data. If **test error is much greater than training error**, then we may risk **overfitting** the data. In this case, we can be more selective about which features to use, further reduce dimensionality (e.g., using fewer PCs), use bagging, choose simpler models, collect more data, or augment the data somehow. If test error and training error are both pretty high, then we may be **underfitting** the data. This is unlikely for a small dataset; say it happens, we can opt for more powerful models, augment features, downsample observations, or use boosting.\n",
    "\n",
    "Error metrics for multiclass classification are similar to those used in binary classification. The most straightforward way is to plot a **confusion matrix** to see which class labels got correctly classified or misclassified. If there is no apparent class imbalance, we can use **accuracy** (what fraction of observations were correctly classified) to evaluate model performance. Accuracy is intuitive but misleading when classes are imbalanced, in which case a lazy classifier that always predicts the majority class will be highly accurate but useless. Metrics such as **recall**, **precision**, or **F1-score** require some tweaks. We can calculate metrics for each class in a binary fashion (e.g., A vs. not A, B vs. not B, C vs. not C, etc.) and average them — this is called **macro-averaging**. Alternatively, we can use raw counts of true positives, false positives, and false negatives to compute **micro-averages**. We can also use **RUC-AOC** in a pairwise fashion (e.g., A vs. B, A vs. C, etc.) to measure the probability that we can tell a random instance from one class or another. If our model churns out not just a class label but probabilities of class assignments, we can compare these probabilities to true labels to compute **log-loss**. This is more precise than the other metrics that I mentioned. A random forest classifier only predicts labels so log-loss doesn't apply but we can use it for multinomial logistic model, for instance. In practice, I'll use multiple metrics to ensure robustness of model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algo + Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1: Fibionacci\n",
    "![fib image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Fibonacci_spiral_34.svg/200px-Fibonacci_spiral_34.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.1\n",
    "Given $n$ where $n \\in \\mathbb{N}$ (i.e., $n$ is an integer and $n > 0$), write a function `fibonacci(n)` that computes the Fibonacci number $F_n$, where $F_n$ is defined by the recurrence relation:\n",
    "\n",
    "$$ F_n = F_{n-1} + F_{n-2}$$\n",
    "\n",
    "with initial conditions of:\n",
    "\n",
    "$$ F_1 = 1,  F_2 = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.2\n",
    "What's the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.3\n",
    "Consider an alternative implementation to compute Fibonacci number $F_n$ and write a new function, `fibonacci2(n)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.4\n",
    "What's the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.5\n",
    "What are some examples of optimizations that could improve computational performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.2: Linked List\n",
    "![ll img](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Singly-linked-list.svg/500px-Singly-linked-list.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.1\n",
    "Consider a [singly linked list](https://en.wikipedia.org/wiki/Linked_list), $L$. Write a function `is_palindrome(L)` that detects if $L$ is a [palindrome](https://en.wikipedia.org/wiki/Palindrome), by returning a bool, `True` or `False`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.2\n",
    "What is the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.3\n",
    "Consider an alternative implementation to detect if L is a palindrome and write a new function, `is_palindrome2(L)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.4\n",
    "What's the complexity of this implementation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.5 \n",
    "What are some examples of optimizations that could improve computational performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prob + Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.1: Finding $\\pi$ in a random uniform?\n",
    "<img src=https://www.epicurus.com/food/recipes/wp-content/uploads/2015/03/Pi-Day.jpg width=\"480\">\n",
    "\n",
    "Given a uniform random generator $[0,1)$ (e.g., use your language's standard libary to generate random value), write a a function `compute_pi` to compute [$\\pi$](https://en.wikipedia.org/wiki/Pi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.2: Making a 6-side die roll a 7?\n",
    "\n",
    "Using a single 6-side die, how can you generate a random number between 1 - 7?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.3: Is normality uniform?\n",
    "\n",
    "<img src=https://rednaxela1618.files.wordpress.com/2014/06/uniformnormal.png width=\"480\">\n",
    "\n",
    "\n",
    "Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.4: Should you pay or should you go?\n",
    "\n",
    "![coin flip](https://lh5.ggpht.com/iwD6MnHeHVAXNBgrO7r4N9MQxxYi6wT9vb0Mqu905zTnNlBciONAA98BqafyjzC06Q=w300)\n",
    "\n",
    "Let’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you $2^{(n-1)}$ US dollars. How much would you pay me to play this game? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.5: Uber vs. Lyft\n",
    "\n",
    "![uber vs lyft](http://usiaffinity.typepad.com/.a/6a01347fc1cb08970c01bb0876bcbe970d-pi)\n",
    "\n",
    "You request 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.6: Pick your prize\n",
    "<img src=https://miro.medium.com/max/1100/1*m5b3O9sE68UCXjLw5oxy2g.png width=\"480\">\n",
    "\n",
    "A prize is placed at random behind one of three doors and you are asked to pick a door. To be concrete, say you always pick door 1. Now the game host chooses one of door 2 or 3, opens it and shows you that it is empty. They then give you the option to keep your picked door or switch to the unopened door. Should you stay or switch if you want to maximize your probability of winning the prize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conceptual ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.1 Why study gradient boosting or neural networks?\n",
    "\n",
    "Consider a regression setting where $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}$. The goal is to come up with a function $f(X): \\mathbb{R}^p \\rightarrow \\mathbb{R}$ that minimizes the squared-error loss $(Y - f(X))^2$. Since X, Y are random variables, we seek to minimize the expectation of the squared error loss as follows\n",
    "\\begin{equation}\n",
    "EPE(f) = \\mathbb{E}\\left[(Y-f(X)^2\\right]\n",
    "\\end{equation}\n",
    "where EPE stands for expected prediction error. One can show that minimizing the expected prediction error leads to the following _regression function_\n",
    "\\begin{equation}\n",
    "f(x) = \\mathbb{E}\\left[Y|X=x\\right]\n",
    "\\end{equation}\n",
    "\n",
    "The goal of any method is to approximate the regression function above, which we denote as $\\hat{f}(x)$. For example, linear regression explicitly assumes that the regression function is approximately linear in its arguments, i.e. $\\hat{f}(x) = x^T\\beta$, while a neural network provides a nonlinear approximation of the regression function. \n",
    "\n",
    "The simplest of all these methods is [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Given $x$ and some neighborhood of $k$ points $N_k(x)$, $\\hat{f}(x)$ is simply the average of all $y_i|x_i \\in N_k(x)$.  Let $N$ denote the training sample size. Under mild regularity conditions on the joint probability distribution $Pr(X, Y)$, one can show that as $N \\rightarrow \\infty$, $k \\rightarrow \\infty$ such that $k/N \\rightarrow 0$, then $\\hat{f}(x) \\rightarrow f(x)$ where $\\rightarrow$ means approaches or goes to. In other words, the k-nearest neighbors algorithm converges to the ideal solution as both the training sample size and number of neighbors increase to infinity.\n",
    "\n",
    "Now given this _universal approximator_, why look any further and research other methods? Please share your thoughts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.2 Model selection and assessment\n",
    "\n",
    "Consider a multiclass classification problem with a large number of features $p \\gg N$, e.g, $p=10000, N=100$. The task is threefold:\n",
    "1. Find a \"good\" subset of features that show strong _univariate_ correlation with class labels\n",
    "2. Using the \"good\" subset, build a multiclass classifier\n",
    "3. Estimate the generalization error of the final model\n",
    "\n",
    "Given this dataset, outline your approach and please be sure to cover the following:\n",
    "- Data splitting\n",
    "- Model selection: either estimating the performance of different classifiers or the same classifier with different hyperparameters\n",
    "- Model assessment: having chosen a classifier, estimating the generalization error\n",
    "\n",
    "Assume all features are numerical, the dataset contains no `NULL`s, outliers, etc. and doesn't require any preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
